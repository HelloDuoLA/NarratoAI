1. 目前大模型输出的max_token固定为4k，是否要根据剧本长度来进行控制呢？有些模型最多好像也只能8k
2. 参考 https://github.com/lukeewin/AudioSeparationGUI/blob/main/top/lukeewin/app.py 做语音识别
    1. 本地搭建的voicesense 好像加不了时间戳，实在不行，可以用阿里云的对象存储+语音识别
2. 功能上接入minimax，minimax生成语音时, 可以将结果转成submaker的形式兼容。字幕为什么会创建失败呢，溯源一下。
3. 将whisper 语音识别模型接入
4. 为什么最后合成视频的视频用不了nvenc加速的？
5. 有时候json会解析失败，得具体看看是什么原因导致失败
6. 多进程剪辑好像有点起色，看看能不能通过添加随机选择其中一张卡进行。但好像还是有点慢，是不是大部分时间都用在了在CPU和GPU之间搬运数据呢？要是直接CPU的话会不会更快
7. 自带的画面解说好像默认会保留原声，其他剪辑方式是自动选择保不保留原声的。 




## 非紧急
1. 测试文本/图像模型的函数和实际的调用不是同一套(因为作者修改大模型调用方式了)



## 笔记
1. 应该对每一个视频和字幕内容(哈希编码)创建一个文件夹, 这样子避免重复提取关键帧
    1. 但会遇到一个问题，就是不同帧提取策略创建的图片数量不同，导致图片数量不一致，后面应该考虑具体策略。

2. 如果字幕能够提取出说话人，那么可以尝试合并同一个说话人的字幕，然后再进行画面理解。还可以先把全部字幕输入给大模型，然后让它给说话人进行身份确认，然后将这些辅助信息给到后续的画面理解大模型。