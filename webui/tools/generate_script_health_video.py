# çºªå½•ç‰‡è„šæœ¬ç”Ÿæˆ
# 
import os
import json
import time
import asyncio
import traceback
import streamlit as st
from loguru import logger
from datetime import datetime
import subprocess
import sys

from pathlib import Path
import re

from app.config import config
from app.utils import utils, video_processor
from webui.tools.base import create_vision_analyzer, get_batch_files, get_batch_timestamps, check_video_config, create_text_analyzer

# è·å–é¡¹ç›®æ ¹ç›®å½•
project_root = Path(__file__).parent.parent.parent

def generate_script_health_video(params, subtitle_path, max_concurrent_analysis=None):
    """
    ç”Ÿæˆ çºªå½•ç‰‡ è§†é¢‘è„šæœ¬
    è¦æ±‚: åŸè§†é¢‘æ— å­—å¹•æ— é…éŸ³
    é€‚åˆåœºæ™¯: çºªå½•ç‰‡ã€åŠ¨ç‰©æç¬‘è§£è¯´ã€è’é‡å»ºé€ ç­‰
    
    Args:
        params: è§†é¢‘å¤„ç†å‚æ•°
        subtitle_path: å­—å¹•æ–‡ä»¶è·¯å¾„
        max_concurrent_analysis: æœ€å¤§å¹¶å‘åˆ†ææ•°é‡ï¼Œé»˜è®¤ä¸º3
                                å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼é…ç½®ï¼š
                                1. ç›´æ¥ä¼ å…¥å‚æ•°
                                2. params.max_concurrent_analysis å±æ€§
                                3. é…ç½®æ–‡ä»¶ä¸­çš„ app.max_concurrent_analysis
                                4. é»˜è®¤å€¼ 3
    """
    progress_bar = st.progress(0)
    status_text = st.empty()

    def update_progress(progress: float, message: str = ""):
        progress_bar.progress(progress/100)
        if message:
            status_text.text(f"ğŸ¬ {message}")
        else:
            status_text.text(f"ğŸ“Š è¿›åº¦: {progress}%")

    def srt_to_list(srt_file, *, ensure_ascii=False):
        # è¯»å–å®Œæ•´æ–‡æœ¬
        text = Path(srt_file).read_text(encoding='utf-8')

        # ç”¨æ­£åˆ™æå–æ¯ä¸€ä¸ªå­—å¹•å—
        pattern = re.compile(
            r'(\d+)\s*\n'                    # åºå·
            r'(\d{2}:\d{2}:\d{2},\d{3} --> \d{2}:\d{2}:\d{2},\d{3})\s*\n'  # æ—¶é—´è½´
            r'([\s\S]*?)(?=\n\d+\s*\n|\Z)',  # å­—å¹•å†…å®¹ï¼ˆéè´ªå©ªï¼Œç›´åˆ°ä¸‹ä¸€ä¸ªåºå·æˆ–ç»“å°¾ï¼‰
            re.MULTILINE
        )

        result = [
            {"timestamp": ts, "subtitle": content.replace('\n', ' ').strip()}
            for _, ts, content in pattern.findall(text)
        ]


        return result
    
    def to_ms(t_str: str) -> int:
        """'HH:MM:SS,mmm' â†’ æ¯«ç§’"""
        h, m, s_ms = t_str.split(':')
        s, ms = s_ms.split('.')
        return int(h) * 3_600_000 + int(m) * 60_000 + int(s) * 1_000 + int(ms)

    def to_hmsf(ms: int) -> str:
        """æ¯«ç§’ â†’ 'HH:MM:SS,mmm'"""
        h, ms = divmod(ms, 3_600_000)
        m, ms = divmod(ms,   60_000)
        s, ms = divmod(ms,    1_000)
        return f'{h:02d}:{m:02d}:{s:02d}.{int(ms):03d}'

    def middle_timestamps(sub_list):
        # å–ä¸­é—´æ—¶é—´å€¼(æš‚æ—¶ä¸éœ€è¦, ä½¿ç”¨calculate_extraction_time)
        mids = []
        for item in sub_list:
            start_str, end_str = item['timestamp'].split(' --> ')
            mid_ms = (to_ms(start_str) + to_ms(end_str)) // 2
            mids.append(to_hmsf(mid_ms))
        return mids
    
    def calculate_extraction_times(start_seconds, end_seconds, num_frames=1):
        """
        è®¡ç®—ä¸€ä¸ªå­—å¹•ç‰‡æ®µå†…è¦æå–çš„å…³é”®å¸§æ—¶é—´ç‚¹
        
        Args:
            start_seconds: å¼€å§‹æ—¶é—´ï¼ˆç§’ï¼‰
            end_seconds: ç»“æŸæ—¶é—´ï¼ˆç§’ï¼‰
            num_frames: è¦æå–çš„å¸§æ•°é‡ï¼Œé»˜è®¤1å¸§ï¼ˆä¸­é—´æ—¶åˆ»ï¼‰
        Returns:
            List[str]: æ—¶é—´ç‚¹åˆ—è¡¨ï¼Œæ ¼å¼ä¸º "HH:MM:SS.mmm"
        """
        duration = end_seconds - start_seconds
        
        if num_frames == 1:
            # å•å¸§ï¼šä½¿ç”¨ä¸­é—´æ—¶åˆ»
            mid_time = start_seconds + duration / 2
            return [utils.format_time(mid_time).replace(',', '.')]
        else:
            # å¤šå¸§ï¼šå‡åŒ€åˆ†å¸ƒåœ¨æ—¶é—´æ®µå†…
            times = []
            for i in range(num_frames):
                if num_frames == 1:
                    time_offset = duration / 2
                else:
                    time_offset = (i / (num_frames - 1)) * duration
                frame_time = start_seconds + time_offset
                times.append(utils.format_time(frame_time).replace(',', '.'))
            return times

    try:
        with st.spinner("æ­£åœ¨ç”Ÿæˆè„šæœ¬..."):
            if not params.video_origin_path:
                st.error("è¯·å…ˆé€‰æ‹©è§†é¢‘æ–‡ä»¶")
                return
            
            if not subtitle_path:
                st.error("è¯·å…ˆé€‰æ‹©å­—å¹•æ–‡ä»¶")
                return
            
            
            """
            1. æ•°æ®ç»“æ„æ„å»º - å­—å¹•ä¿¡æ¯è§£æ
            """
            update_progress(5, "æ­£åœ¨è§£æå­—å¹•æ–‡ä»¶...")
            
            # å°†å­—å¹•æ–‡ä»¶è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼ï¼š[{'timestamp': 'æ—¶é—´æ®µ', 'subtitle': 'å­—å¹•å†…å®¹'}, ...]
            video_clips = srt_to_list(subtitle_path)
            
            if not video_clips:
                st.error("å­—å¹•æ–‡ä»¶è§£æå¤±è´¥æˆ–ä¸ºç©ºï¼Œè¯·æ£€æŸ¥å­—å¹•æ–‡ä»¶æ ¼å¼")
                return
            
            logger.info(f"æˆåŠŸè§£æå­—å¹•æ–‡ä»¶ï¼Œå…± {len(video_clips)} æ¡å­—å¹•")
            
            
            # æ„å»ºå­—å¹•ä¸å…³é”®å¸§çš„æ˜ å°„æ•°æ®ç»“æ„
            update_progress(7, "æ­£åœ¨æ„å»ºå­—å¹•æ•°æ®ç»“æ„...")
            
            subtitle_keyframe_data = []
            
            # é€ä¸ªå¤„ç†å­—å¹•ç‰‡æ®µ
            for i, clip in enumerate(video_clips):
                # æ›´æ–°è¿›åº¦
                current_progress = 7 + (i / len(video_clips)) * 3  # 7%-10%çš„è¿›åº¦èŒƒå›´
                update_progress(current_progress, f"æ­£åœ¨å¤„ç†ç¬¬{i+1}/{len(video_clips)}ä¸ªå­—å¹•ç‰‡æ®µ...")
                
                try:
                    # è§£ææ—¶é—´æˆ³
                    start_str, end_str = clip['timestamp'].split(' --> ')
                    start_seconds = utils.time_to_seconds(start_str.replace(',', '.'))
                    end_seconds = utils.time_to_seconds(end_str.replace(',', '.'))
                    duration = end_seconds - start_seconds
                    
                    # æ„å»ºæ•°æ®ç»“æ„
                    data_item = {
                        "index": i,
                        "subtitle_text": clip['subtitle'],
                        "timestamp": clip['timestamp'],  # ä¿æŒåŸå§‹SRTæ—¶é—´æˆ³æ ¼å¼ "00:01:30,500 --> 00:01:35,200"
                        "start_seconds": start_seconds,
                        "end_seconds": end_seconds,
                        "duration": duration,
                        "extraction_times": calculate_extraction_times(start_seconds, end_seconds, num_frames=1),  # å½“å‰é»˜è®¤æå–1å¸§
                        "keyframe_paths": [],  # å­˜å‚¨å¯¹åº”çš„å…³é”®å¸§è·¯å¾„åˆ—è¡¨
                        "scene_description": None,  # ç¨åé€šè¿‡å¤šæ¨¡æ€å¤§æ¨¡å‹å¡«å……
                    }
                    
                    subtitle_keyframe_data.append(data_item)
                    
                except Exception as e:
                    logger.error(f"å¤„ç†ç¬¬{i+1}ä¸ªç‰‡æ®µæ—¶å‡ºé”™: {e}")
                    raise RuntimeError(f"å¤„ç†ç‰‡æ®µæ—¶å‡ºé”™: {e}")
            
            logger.info(f"æ„å»ºå­—å¹•-å…³é”®å¸§æ˜ å°„æ•°æ®ç»“æ„å®Œæˆï¼Œå…± {len(subtitle_keyframe_data)} ä¸ªç‰‡æ®µ")
            
            """
            2. æå–å…³é”®å¸§å¹¶åŒ¹é…å­—å¹•
            """
            update_progress(10, "æ­£åœ¨æå–å…³é”®å¸§...")

            # åˆ›å»ºä¸´æ—¶ç›®å½•ç”¨äºå­˜å‚¨å…³é”®å¸§
            # !ä¸è¦æ”¾åˆ°ä¸´æ—¶æ–‡ä»¶å¤¹äº†ï¼Œæ”¾å›ºå®šä½ç½®
            keyframes_dir = os.path.join(utils.temp_dir(), "keyframes")
            video_hash = utils.md5(params.video_origin_path + str(os.path.getmtime(params.video_origin_path)) + subtitle_path + str(os.path.getmtime(subtitle_path)) +  "health_video")
            video_keyframes_dir = os.path.join(keyframes_dir, video_hash)

            # æ£€æŸ¥æ˜¯å¦å·²ç»æå–è¿‡å…³é”®å¸§
            keyframe_files = []
            subtitle_keyframe_match_file = os.path.join(video_keyframes_dir, "subtitle_keyframe_match.json")
            logger.info(f"å…³é”®å¸§å­˜å‚¨æ–‡ä»¶å¤¹è·¯å¾„ {video_keyframes_dir}")
            
            if os.path.exists(video_keyframes_dir) and os.path.exists(subtitle_keyframe_match_file):
                # ä»ç¼“å­˜çš„åŒ¹é…æ–‡ä»¶ä¸­è¯»å–æ•°æ®ç»“æ„
                try:
                    with open(subtitle_keyframe_match_file, 'r', encoding='utf-8') as f:
                        cached_data = json.load(f)
                    
                    # éªŒè¯ç¼“å­˜æ•°æ®çš„å®Œæ•´æ€§
                    if len(cached_data) == len(subtitle_keyframe_data):
                        # æ›´æ–° subtitle_keyframe_data ä½¿ç”¨ç¼“å­˜çš„å…³é”®å¸§è·¯å¾„
                        for i, cached_item in enumerate(cached_data):
                            if i < len(subtitle_keyframe_data):
                                subtitle_keyframe_data[i]['keyframe_paths'] = cached_item['keyframe_paths']
                        
                        # ç»Ÿè®¡ç¼“å­˜çš„å…³é”®å¸§æ–‡ä»¶
                        total_cached_keyframes = sum(len(item['keyframe_paths']) for item in subtitle_keyframe_data)
                        
                        logger.info(f"ä½¿ç”¨å·²ç¼“å­˜çš„å…³é”®å¸§åŒ¹é…æ•°æ®: {subtitle_keyframe_match_file}")
                        st.info(f"âœ… ä½¿ç”¨å·²ç¼“å­˜å…³é”®å¸§åŒ¹é…æ•°æ®ï¼Œå…± {total_cached_keyframes} å¸§")
                        update_progress(20, f"ä½¿ç”¨å·²ç¼“å­˜å…³é”®å¸§åŒ¹é…æ•°æ®ï¼Œå…± {total_cached_keyframes} å¸§")
                        
                        # æ ‡è®°ä¸ºä½¿ç”¨ç¼“å­˜
                        using_cached_data = True
                    else:
                        logger.warning(f"ç¼“å­˜æ•°æ®é•¿åº¦ä¸åŒ¹é…ï¼Œé‡æ–°æå–å…³é”®å¸§")
                        using_cached_data = False
                except Exception as cache_error:
                    logger.warning(f"è¯»å–ç¼“å­˜åŒ¹é…æ•°æ®å¤±è´¥: {cache_error}ï¼Œé‡æ–°æå–å…³é”®å¸§")
                    using_cached_data = False
            else:
                using_cached_data = False

            # å¦‚æœæ²¡æœ‰ç¼“å­˜çš„å…³é”®å¸§ï¼Œåˆ™è¿›è¡Œæå–
            if not using_cached_data:
                try:
                    # ç¡®ä¿ç›®å½•å­˜åœ¨
                    os.makedirs(video_keyframes_dir, exist_ok=True)

                    # æ˜¾ç¤ºè§†é¢‘ä¿¡æ¯
                    temp_processor = video_processor.VideoProcessor(params.video_origin_path)
                    st.info(f"ğŸ“¹ è§†é¢‘ä¿¡æ¯: {temp_processor.width}x{temp_processor.height}, {temp_processor.fps:.1f}fps, {temp_processor.duration:.1f}ç§’")

                    # æå‰ä¿å­˜å­—å¹•-å…³é”®å¸§åŒ¹é…æ•°æ®ï¼Œä¾›ç‹¬ç«‹å·¥å…·ä½¿ç”¨
                    temp_match_file = os.path.join(video_keyframes_dir, "subtitle_keyframe_match_input.json")
                    
                    # é¢„å…ˆç”Ÿæˆæ‰€æœ‰å…³é”®å¸§æ–‡ä»¶è·¯å¾„ï¼ˆè¿™æ ·å°±ä¸éœ€è¦åç»­è¯»å–æ›´æ–°æ–‡ä»¶ï¼‰
                    update_progress(15, "æ­£åœ¨é¢„å¤„ç†å…³é”®å¸§æ–‡ä»¶è·¯å¾„...")
                    total_expected = 0
                    
                    for i, data_item in enumerate(subtitle_keyframe_data):
                        for j, extraction_time in enumerate(data_item['extraction_times']):
                            # è®¡ç®—å…³é”®å¸§æ–‡ä»¶åï¼ˆä¸ç‹¬ç«‹å·¥å…·ä¿æŒä¸€è‡´ï¼‰
                            time_str = extraction_time.replace(':', '').replace('.', '')
                            keyframe_filename = f"segment_{i + 1}_keyframe_{time_str}.jpg"
                            keyframe_path = os.path.join(video_keyframes_dir, keyframe_filename)
                            
                            # é¢„å…ˆå°†è·¯å¾„æ·»åŠ åˆ°æ•°æ®ç»“æ„ä¸­
                            data_item['keyframe_paths'].append(keyframe_path)
                            total_expected += 1
                    
                    logger.info(f"é¢„å¤„ç†å®Œæˆï¼Œé¢„æœŸæå– {total_expected} ä¸ªå…³é”®å¸§")
                    
                    with open(temp_match_file, 'w', encoding='utf-8') as f:
                        # åˆ›å»ºä¸€ä¸ªå¯åºåˆ—åŒ–çš„ç‰ˆæœ¬ï¼ˆç”¨äºç‹¬ç«‹å·¥å…·ï¼‰
                        serializable_data = []
                        for item in subtitle_keyframe_data:
                            serializable_item = {
                                "index": item["index"],
                                "subtitle_text": item["subtitle_text"],
                                "timestamp": item["timestamp"],
                                "duration": item["duration"],
                                "extraction_times": item["extraction_times"],
                                "keyframe_paths": [],  # ç‹¬ç«‹å·¥å…·ä¼šå¡«å……è¿™ä¸ªï¼Œä½†æˆ‘ä»¬å·²ç»åœ¨ä¸»ç»“æ„ä¸­é¢„è®¾äº†
                                "has_keyframes": False
                            }
                            serializable_data.append(serializable_item)
                        json.dump(serializable_data, f, ensure_ascii=False, indent=2)
                    
                    logger.info(f"å­—å¹•-å…³é”®å¸§åŒ¹é…æ•°æ®å·²ä¿å­˜åˆ°: {temp_match_file}")

                    # ä½¿ç”¨ç‹¬ç«‹çš„å…³é”®å¸§æå–å·¥å…·
                    update_progress(16, f"æ­£åœ¨å¯åŠ¨ç‹¬ç«‹å…³é”®å¸§æå–å·¥å…·ï¼ˆå…±{total_expected}ä¸ªå…³é”®å¸§ï¼‰...")
                    
                    # æ„å»ºå‘½ä»¤è¡Œå‚æ•°
                    extract_tool_path = os.path.join(project_root, "tools", "extract_keyframes_simple.py")
                    if not os.path.exists(extract_tool_path):
                        extract_tool_path = os.path.join(os.path.dirname(__file__), "..", "..", "tools", "extract_keyframes_simple.py")
                    
                    cmd = [
                        sys.executable,
                        extract_tool_path,
                        "--video_path", params.video_origin_path,
                        "--subtitle_keyframe_match", temp_match_file,
                        "--output_dir", video_keyframes_dir,
                        "--max_workers", "20",
                        "--log_level", "INFO"
                    ]
                    
                    logger.info(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
                    
                    # å¯åŠ¨ç‹¬ç«‹è¿›ç¨‹
                    process = subprocess.Popen(
                        cmd,
                        stdout=subprocess.PIPE,
                        stderr=subprocess.PIPE,
                        text=True,
                        cwd=os.path.dirname(extract_tool_path) if os.path.exists(os.path.dirname(extract_tool_path)) else None
                    )
                    
                    # ç®€åŒ–çš„è¿›åº¦ç›‘å¬ï¼šç›´æ¥ç»Ÿè®¡æ–‡ä»¶æ•°é‡
                    update_progress(17, "å…³é”®å¸§æå–å·¥å…·å·²å¯åŠ¨ï¼Œæ­£åœ¨ç›‘å¬æ–‡ä»¶ç”Ÿæˆ...")
                    
                    start_time = time.time()
                    timeout = 3000  # 5åˆ†é’Ÿè¶…æ—¶
                    last_count = 0
                    check_interval = 2  # æ¯2ç§’æ£€æŸ¥ä¸€æ¬¡
                    
                    while True:
                        # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦è¿˜åœ¨è¿è¡Œ
                        if process.poll() is not None:
                            # è¿›ç¨‹å·²ç»“æŸï¼Œæœ€åæ£€æŸ¥ä¸€æ¬¡æ–‡ä»¶æ•°é‡
                            break
                        
                        # æ£€æŸ¥è¶…æ—¶
                        if time.time() - start_time > timeout:
                            logger.error("å…³é”®å¸§æå–è¶…æ—¶")
                            process.terminate()
                            try:
                                process.wait(timeout=10)
                            except subprocess.TimeoutExpired:
                                process.kill()
                            raise Exception("å…³é”®å¸§æå–è¶…æ—¶ï¼Œè¯·æ£€æŸ¥è§†é¢‘æ–‡ä»¶æˆ–å‡å°‘æå–æ•°é‡")
                        
                        # ç»Ÿè®¡å·²ç”Ÿæˆçš„å…³é”®å¸§æ–‡ä»¶æ•°é‡
                        try:
                            if os.path.exists(video_keyframes_dir):
                                # åªç»Ÿè®¡å®é™…çš„å…³é”®å¸§æ–‡ä»¶ï¼ˆ.jpgä¸”åŒ…å«segment_å’Œkeyframe_ï¼‰
                                all_files = os.listdir(video_keyframes_dir)
                                keyframe_files = [
                                    f for f in all_files 
                                    if f.endswith('.jpg') and 'segment_' in f and 'keyframe_' in f
                                ]
                                current_count = len(keyframe_files)
                                
                                if current_count != last_count:
                                    # è®¡ç®—è¿›åº¦ç™¾åˆ†æ¯” (17%-20%çš„è¿›åº¦èŒƒå›´)
                                    progress_percent = 17 + (current_count / total_expected) * 3
                                    update_progress(progress_percent, f"å·²æå– {current_count}/{total_expected} ä¸ªå…³é”®å¸§")
                                    last_count = current_count
                                    
                                    logger.debug(f"å½“å‰å·²æå– {current_count}/{total_expected} ä¸ªå…³é”®å¸§")
                        
                        except Exception as e:
                            logger.warning(f"ç»Ÿè®¡å…³é”®å¸§æ–‡ä»¶å¤±è´¥: {e}")
                        
                        # ç­‰å¾…ä¸€æ®µæ—¶é—´å†æ£€æŸ¥
                        time.sleep(check_interval)
                    
                    # ç­‰å¾…è¿›ç¨‹å®Œæˆ
                    stdout, stderr = process.communicate(timeout=30)
                    
                    if process.returncode != 0:
                        logger.error(f"å…³é”®å¸§æå–è¿›ç¨‹å¤±è´¥ï¼Œè¿”å›ç : {process.returncode}")
                        logger.error(f"é”™è¯¯è¾“å‡º: {stderr}")
                        raise Exception(f"å…³é”®å¸§æå–å¤±è´¥: {stderr}")
                    
                    logger.info("å…³é”®å¸§æå–è¿›ç¨‹å®Œæˆ")
                    
                    # æœ€ç»ˆç»Ÿè®¡ï¼šæ£€æŸ¥å“ªäº›æ–‡ä»¶å®é™…ç”Ÿæˆäº†
                    successful_extractions = 0
                    failed_extractions = 0
                    
                    for data_item in subtitle_keyframe_data:
                        # æ£€æŸ¥é¢„å®šä¹‰è·¯å¾„ä¸­å“ªäº›æ–‡ä»¶å®é™…å­˜åœ¨
                        existing_paths = []
                        for keyframe_path in data_item['keyframe_paths']:
                            if os.path.exists(keyframe_path):
                                existing_paths.append(keyframe_path)
                                successful_extractions += 1
                            else:
                                failed_extractions += 1
                        
                        # æ›´æ–°ä¸ºå®é™…å­˜åœ¨çš„æ–‡ä»¶è·¯å¾„
                        data_item['keyframe_paths'] = existing_paths
                    
                    if successful_extractions == 0:
                        # æ£€æŸ¥ç›®å½•ä¸­æ˜¯å¦æœ‰å…¶ä»–æ–‡ä»¶
                        all_files = os.listdir(video_keyframes_dir) if os.path.exists(video_keyframes_dir) else []
                        logger.error(f"å…³é”®å¸§ç›®å½•å†…å®¹: {all_files}")
                        raise Exception("æœªæå–åˆ°ä»»ä½•å…³é”®å¸§æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è§†é¢‘æ–‡ä»¶æ ¼å¼")

                    update_progress(20, f"å…³é”®å¸§æå–å®Œæˆï¼ŒæˆåŠŸ {successful_extractions}/{total_expected} å¸§")
                    
                    if failed_extractions > 0:
                        st.warning(f"âš ï¸ æœ‰ {failed_extractions} ä¸ªå…³é”®å¸§æå–å¤±è´¥ï¼Œå°†ç»§ç»­å¤„ç†æˆåŠŸçš„ {successful_extractions} ä¸ª")
                    else:
                        st.success(f"âœ… æˆåŠŸæå– {successful_extractions} ä¸ªå…³é”®å¸§")

                except Exception as e:
                    # å¦‚æœæå–å¤±è´¥ï¼Œæ¸…ç†é¢„è®¾çš„è·¯å¾„ï¼ˆå› ä¸ºæ–‡ä»¶å¯èƒ½æ²¡æœ‰å®é™…ç”Ÿæˆï¼‰
                    for data_item in subtitle_keyframe_data:
                        data_item['keyframe_paths'] = []
                    
                    # æ¸…ç†åˆ›å»ºçš„ç›®å½•
                    try:
                        if os.path.exists(video_keyframes_dir):
                            import shutil
                            shutil.rmtree(video_keyframes_dir)
                    except Exception as cleanup_err:
                        logger.error(f"æ¸…ç†å¤±è´¥çš„å…³é”®å¸§ç›®å½•æ—¶å‡ºé”™: {cleanup_err}")

                    raise Exception(f"å…³é”®å¸§æå–å¤±è´¥: {str(e)}")

            """
            3. å…³é”®å¸§ä¸å­—å¹•æ•°æ®éªŒè¯å’Œæ•´ç†
            """
            update_progress(25, "æ­£åœ¨éªŒè¯å…³é”®å¸§ä¸å­—å¹•æ•°æ®...")
            
            # ç»Ÿè®¡å…³é”®å¸§æå–æƒ…å†µ
            segments_with_keyframes = [item for item in subtitle_keyframe_data if len(item['keyframe_paths']) > 0]
            segments_without_keyframes = [item for item in subtitle_keyframe_data if len(item['keyframe_paths']) == 0]
            
            total_keyframes = sum(len(item['keyframe_paths']) for item in subtitle_keyframe_data)
            
            logger.info(f"æ•°æ®éªŒè¯å®Œæˆï¼Œå…± {len(subtitle_keyframe_data)} ä¸ªå­—å¹•ç‰‡æ®µ")
            logger.info(f"å…¶ä¸­ {len(segments_with_keyframes)} ä¸ªç‰‡æ®µæœ‰å…³é”®å¸§ï¼ˆå…± {total_keyframes} å¸§ï¼‰ï¼Œ{len(segments_without_keyframes)} ä¸ªç‰‡æ®µæ— å…³é”®å¸§")
            
            # å¯¹æ²¡æœ‰å…³é”®å¸§çš„ç‰‡æ®µå‘å‡ºè­¦å‘Š
            if segments_without_keyframes:
                st.warning(f"âš ï¸ æœ‰ {len(segments_without_keyframes)} ä¸ªå­—å¹•ç‰‡æ®µæ²¡æœ‰æˆåŠŸæå–åˆ°å…³é”®å¸§ï¼Œå°†ä»…ä½¿ç”¨å­—å¹•æ–‡æœ¬è¿›è¡Œåˆ†æ")
                for item in segments_without_keyframes:
                    logger.warning(f"å­—å¹•ç‰‡æ®µ {item['index']+1} æ— å…³é”®å¸§: {item['subtitle_text'][:30]}...")
            
            st.info(f"ğŸ“Š æ•°æ®éªŒè¯å®Œæˆï¼Œå…± {len(subtitle_keyframe_data)} ä¸ªå­—å¹•ç‰‡æ®µï¼Œå…¶ä¸­ {len(segments_with_keyframes)} ä¸ªæœ‰å…³é”®å¸§")
            
            # ä¸ºæœ‰å…³é”®å¸§çš„æ•°æ®æ·»åŠ è°ƒè¯•ä¿¡æ¯
            for data_item in segments_with_keyframes:
                for j, keyframe_path in enumerate(data_item['keyframe_paths']):
                    filename = os.path.basename(keyframe_path)
                    logger.debug(f"å­—å¹•{data_item['index']+1}-å¸§{j+1}: {data_item['subtitle_text'][:20]}... -> {filename}")
            
            # åªæœ‰åœ¨é‡æ–°æå–å…³é”®å¸§æ—¶æ‰ä¿å­˜åŒ¹é…æ•°æ®åˆ°å…³é”®å¸§ç›®å½•
            if not using_cached_data:
                subtitle_keyframe_match_file = os.path.join(video_keyframes_dir, "subtitle_keyframe_match.json")
                with open(subtitle_keyframe_match_file, 'w', encoding='utf-8') as f:
                    # åˆ›å»ºä¸€ä¸ªå¯åºåˆ—åŒ–çš„ç‰ˆæœ¬
                    serializable_data = []
                    for item in subtitle_keyframe_data:
                        serializable_item = {
                            "index": item["index"],
                            "subtitle_text": item["subtitle_text"],
                            "timestamp": item["timestamp"],  # åŸå§‹SRTæ—¶é—´æˆ³æ ¼å¼
                            "duration": item["duration"],
                            "extraction_times": item["extraction_times"],
                            "keyframe_paths": item["keyframe_paths"],
                            "has_keyframes": len(item["keyframe_paths"]) > 0
                        }
                        serializable_data.append(serializable_item)
                    json.dump(serializable_data, f, ensure_ascii=False, indent=2)
                
                logger.info(f"å­—å¹•-å…³é”®å¸§åŒ¹é…æ•°æ®å·²ä¿å­˜åˆ°: {subtitle_keyframe_match_file}")

            # ä¿æŒæ‰€æœ‰æ•°æ®ï¼ŒåŒ…æ‹¬æ²¡æœ‰å…³é”®å¸§çš„ç‰‡æ®µ
            
            """
            4. ç”»é¢ç†è§£ä¸å‰§æƒ…æ¢³ç†ï¼ˆé€ä¸ªå­—å¹•ç‰‡æ®µåˆ†æï¼‰
            """
            vision_llm_provider = st.session_state.get('vision_llm_providers').lower()
            logger.info(f"ä½¿ç”¨ {vision_llm_provider.upper()} è¿›è¡Œè§†è§‰åˆ†æ")

            try:
                # ===================åˆå§‹åŒ–è§†è§‰åˆ†æå™¨===================
                update_progress(35, "æ­£åœ¨åˆå§‹åŒ–è§†è§‰åˆ†æå™¨...")

                # ä»é…ç½®ä¸­è·å–ç›¸å…³é…ç½®
                vision_api_key = st.session_state.get(f'vision_{vision_llm_provider}_api_key')
                vision_model = st.session_state.get(f'vision_{vision_llm_provider}_model_name')
                vision_base_url = st.session_state.get(f'vision_{vision_llm_provider}_base_url')

                # åˆ›å»ºè§†è§‰åˆ†æå™¨å®ä¾‹
                analyzer = create_vision_analyzer(
                    provider=vision_llm_provider,
                    api_key=vision_api_key,
                    model=vision_model,
                    base_url=vision_base_url
                )

                update_progress(40, "æ­£åœ¨è¿›è¡Œç”»é¢ç†è§£ä¸å‰§æƒ…æ¢³ç†...")

                # ===================åˆ›å»ºå¼‚æ­¥äº‹ä»¶å¾ªç¯===================
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)

                # ===================å¹¶è¡Œåˆ†æå­—å¹•ç‰‡æ®µ===================
                async def parallel_analyze_segments():
                    # ç¡®ä¿å¹¶å‘æ•°é‡åœ¨åˆç†èŒƒå›´å†…
                    concurrent_tasks = st.session_state.get('max_concurrent_LLM_requests')  
                    
                    logger.info(f"ä½¿ç”¨å¹¶å‘åˆ†æï¼Œæœ€å¤§å¹¶å‘æ•°: {concurrent_tasks}")
                    st.info(f"ğŸ”„ é…ç½®å¹¶å‘åˆ†æ: {concurrent_tasks} ä¸ªä»»åŠ¡å¹¶è¡Œå¤„ç†")

                    # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
                    semaphore = asyncio.Semaphore(concurrent_tasks)
                    
                    # ç”¨äºè·Ÿè¸ªè¿›åº¦çš„å…±äº«å˜é‡
                    completed_count = {'value': 0}
                    total_tasks = len(subtitle_keyframe_data)
                    
                    async def analyze_single_segment(i, data_item):
                        """å¼‚æ­¥åˆ†æå•ä¸ªå­—å¹•ç‰‡æ®µ"""
                        async with semaphore:  # æ§åˆ¶å¹¶å‘æ•°
                            try:
                                # è·å–å­—å¹•ç‰‡æ®µä¿¡æ¯
                                subtitle_text = data_item['subtitle_text']
                                timestamp = data_item['timestamp'] 
                                keyframe_paths = data_item['keyframe_paths']
                                
                                # æ„å»ºåˆ†æprompt
                                if keyframe_paths:
                                    # æœ‰å…³é”®å¸§ï¼šç”»é¢+å­—å¹•åˆ†æ
                                    analysis_prompt = f"""
                                        æˆ‘æä¾›äº† {len(keyframe_paths)} å¼ è§†é¢‘å¸§å’Œå¯¹åº”çš„å­—å¹•å†…å®¹ï¼Œè¯·è¿›è¡Œç”»é¢ç†è§£ä¸å‰§æƒ…æ¢³ç†ã€‚

                                        å­—å¹•æ—¶é—´æ®µï¼š{timestamp}
                                        å­—å¹•å†…å®¹ï¼š"{subtitle_text}"

                                        è¯·ä»”ç»†åˆ†æè§†é¢‘å¸§çš„å†…å®¹ï¼Œå¹¶ç»“åˆå­—å¹•æ–‡æœ¬ï¼Œå®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š
                                        1. ç”»é¢ç†è§£ï¼šè¯¦ç»†æè¿°ç”»é¢ä¸­çš„ä¸»è¦å†…å®¹ã€äººç‰©ã€åŠ¨ä½œã€åœºæ™¯ã€æƒ…æ„Ÿè¡¨è¾¾
                                        2. å‰§æƒ…æ¢³ç†ï¼šåŸºäºç”»é¢å’Œå­—å¹•ï¼Œç†è§£è¿™ä¸ªç‰‡æ®µåœ¨æ•´ä¸ªæ•…äº‹ä¸­çš„ä½œç”¨å’Œæ„ä¹‰

                                        è¯·åŠ¡å¿…ä½¿ç”¨ JSON æ ¼å¼è¾“å‡ºä½ çš„ç»“æœï¼š
                                        {{
                                            "scene_description": "è¯¦ç»†çš„ç”»é¢æè¿°ï¼ŒåŒ…å«ä¸»è¦å†…å®¹ã€äººç‰©ã€åŠ¨ä½œå’Œåœºæ™¯",
                                            "emotion_tone": "ç”»é¢å’Œå­—å¹•ä¼ è¾¾çš„æƒ…æ„Ÿè‰²å½©",
                                            "key_elements": ["åˆ—å‡º", "é‡è¦çš„", "è§†è§‰å…ƒç´ "],
                                            "plot_analysis": "è¿™ä¸ªç‰‡æ®µåœ¨å‰§æƒ…ä¸­çš„ä½œç”¨å’Œæ„ä¹‰",
                                            "content_summary": "å¯¹è¿™ä¸ªç‰‡æ®µå†…å®¹çš„ç®€æ´æ€»ç»“"
                                        }}
                                        è¯·åªè¿”å› JSON å­—ç¬¦ä¸²ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–è§£é‡Šæ€§æ–‡å­—ã€‚
                                    """                        
                                    # è¿›è¡Œè§†è§‰+æ–‡æœ¬åˆ†æ
                                    try:
                                        segment_results = await analyzer.analyze_image_with_subtitle(
                                            images=keyframe_paths,
                                            prompt=analysis_prompt,
                                            index=i
                                        )

                                        # å­˜åœ¨å“åº”
                                        if segment_results and len(segment_results) > 0:
                                            response_text = segment_results[0]['response']
                                            
                                            # è§£æJSONå“åº”
                                            try:
                                                analysis_data = parse_and_fix_json(response_text)
                                                
                                                if analysis_data:
                                                    # ä¿å­˜åˆ†æç»“æœåˆ°æ•°æ®ç»“æ„ä¸­
                                                    data_item['scene_description'] = analysis_data.get('scene_description', '')
                                                    data_item['emotion_tone'] = analysis_data.get('emotion_tone', '')
                                                    data_item['key_elements'] = analysis_data.get('key_elements', [])
                                                    data_item['plot_analysis'] = analysis_data.get('plot_analysis', '')
                                                    data_item['content_summary'] = analysis_data.get('content_summary', '')
                                                    
                                                    logger.info(f"å­—å¹•ç‰‡æ®µ {i+1} ç”»é¢ç†è§£å®Œæˆ")
                                                else:
                                                    logger.error(f"å­—å¹•ç‰‡æ®µ {i+1} JSONè§£æå¤±è´¥")
                                                    # ä½¿ç”¨åŸå§‹å“åº”ä½œä¸ºæè¿°
                                                    data_item['scene_description'] = response_text[:200] + "..."
                                                    data_item['content_summary'] = f"åŸºäºå­—å¹•ï¼š{subtitle_text}"
                                                
                                            except Exception as parse_error:
                                                logger.error(f"è§£æå­—å¹•ç‰‡æ®µ {i+1} çš„åˆ†æç»“æœå¤±è´¥: {parse_error}")
                                                # ä½¿ç”¨åŸå§‹å“åº”ä½œä¸ºæè¿°
                                                data_item['scene_description'] = response_text[:200] + "..."
                                                data_item['content_summary'] = f"åŸºäºå­—å¹•ï¼š{subtitle_text}"
                                        else:
                                            logger.warning(f"å­—å¹•ç‰‡æ®µ {i+1} åˆ†æå¤±è´¥ï¼Œæœªè¿”å›ç»“æœ")
                                            data_item['content_summary'] = f"åŸºäºå­—å¹•ï¼š{subtitle_text}"
                                            
                                    except Exception as segment_error:
                                        logger.error(f"å­—å¹•ç‰‡æ®µ {i+1} åˆ†æå‡ºé”™: {segment_error}")
                                        data_item['content_summary'] = f"åŸºäºå­—å¹•ï¼š{subtitle_text}"
                                else:
                                    # æ²¡æœ‰å…³é”®å¸§ï¼šåŸºäºå­—å¹•å†…å®¹è¿›è¡Œæ–‡æœ¬åˆ†æï¼ˆä½¿ç”¨è§†è§‰åˆ†æå™¨çš„æ–‡æœ¬å¤„ç†èƒ½åŠ›ï¼‰
                                    logger.info(f"å­—å¹•ç‰‡æ®µ {i+1} æ²¡æœ‰å…³é”®å¸§ï¼ŒåŸºäºå­—å¹•è¿›è¡Œæ–‡æœ¬å†…å®¹åˆ†æ")
                                    
                                    # æ„å»ºåŸºäºå­—å¹•çš„åˆ†æprompt
                                    text_analysis_prompt = f"""
                                        åŸºäºä»¥ä¸‹å­—å¹•å†…å®¹ï¼Œè¯·è¿›è¡Œæ·±åº¦æ–‡æœ¬åˆ†æå’Œå‰§æƒ…ç†è§£ã€‚

                                        å­—å¹•æ—¶é—´æ®µï¼š{timestamp}
                                        å­—å¹•å†…å®¹ï¼š"{subtitle_text}"

                                        è™½ç„¶æ²¡æœ‰ç”»é¢ä¿¡æ¯ï¼Œä½†è¯·åŸºäºå­—å¹•æ–‡æœ¬å†…å®¹ï¼Œå®Œæˆä»¥ä¸‹åˆ†æï¼š
                                        1. å†…å®¹ç†è§£ï¼šä»å­—å¹•æ¨æµ‹å¯èƒ½çš„ç”»é¢åœºæ™¯ã€äººç‰©åŠ¨ä½œã€ç¯å¢ƒæè¿°
                                        2. æƒ…æ„Ÿåˆ†æï¼šåˆ†æå­—å¹•ä¼ è¾¾çš„æƒ…æ„Ÿè‰²å½©å’Œè¯­æ°”ï¼ˆå¦‚ï¼šç§¯æã€æ¶ˆæã€ä¸­æ€§ã€å…´å¥‹ã€å¹³é™ã€ç´§å¼ ç­‰ï¼‰
                                        3. å‰§æƒ…æ¨æµ‹ï¼šæ ¹æ®å­—å¹•å†…å®¹æ¨æµ‹è¿™ä¸ªç‰‡æ®µåœ¨æ•´ä½“æ•…äº‹ä¸­çš„ä½œç”¨
                                        4. å…³é”®ä¿¡æ¯æå–ï¼šè¯†åˆ«å­—å¹•ä¸­çš„é‡è¦ä¿¡æ¯ç‚¹

                                        è¯·åŠ¡å¿…ä½¿ç”¨ JSON æ ¼å¼è¾“å‡ºä½ çš„ç»“æœï¼š
                                        {{
                                            "scene_description": "åŸºäºå­—å¹•æ¨æµ‹çš„å¯èƒ½ç”»é¢åœºæ™¯æè¿°",
                                            "emotion_tone": "å­—å¹•ä¼ è¾¾çš„å…·ä½“æƒ…æ„Ÿè‰²å½©ï¼ˆå¦‚ï¼šç§¯æå‘ä¸Šã€è½»æ¾æ„‰å¿«ã€ä¸¥è‚ƒè®¤çœŸã€ç´§å¼ åˆºæ¿€ç­‰ï¼‰",
                                            "key_elements": ["ä»å­—å¹•ä¸­", "æå–çš„", "å…³é”®ä¿¡æ¯ç‚¹"],
                                            "plot_analysis": "è¿™ä¸ªç‰‡æ®µåœ¨å‰§æƒ…ä¸­çš„æ¨æµ‹ä½œç”¨å’Œæ„ä¹‰",
                                            "content_summary": "å¯¹è¿™ä¸ªå­—å¹•ç‰‡æ®µçš„æ·±åº¦ç†è§£æ€»ç»“"
                                        }}

                                        è¯·åªè¿”å› JSON å­—ç¬¦ä¸²ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–è§£é‡Šæ€§æ–‡å­—ã€‚
                                    """
                                    
                                    # ä½¿ç”¨è§†è§‰åˆ†æå™¨è¿›è¡Œæ–‡æœ¬åˆ†æï¼ˆä¸ä¼ å…¥å›¾ç‰‡ï¼Œåªåˆ†ææ–‡æœ¬ï¼‰
                                    text_segment_results = await analyzer.analyze_images(
                                        images=[],  # ç©ºå›¾ç‰‡åˆ—è¡¨ï¼Œåªè¿›è¡Œæ–‡æœ¬åˆ†æ
                                        prompt=text_analysis_prompt,
                                        batch_size=1
                                    )
                                    
                                    if text_segment_results and len(text_segment_results) > 0:
                                        text_response = text_segment_results[0]['response']
                                        
                                        # è§£æJSONå“åº”
                                        try:
                                            text_analysis_data = parse_and_fix_json(text_response)
                                            if text_analysis_data:
                                                # ä¿å­˜åˆ†æç»“æœåˆ°æ•°æ®ç»“æ„ä¸­
                                                data_item['scene_description'] = text_analysis_data.get('scene_description', f"åŸºäºå­—å¹•æ¨æµ‹ï¼š{subtitle_text}")
                                                data_item['emotion_tone'] = text_analysis_data.get('emotion_tone', 'ä¸­æ€§')
                                                data_item['key_elements'] = text_analysis_data.get('key_elements', [])
                                                data_item['plot_analysis'] = text_analysis_data.get('plot_analysis', f"åŸºäºå­—å¹•å†…å®¹æ¨æµ‹ï¼š{subtitle_text}")
                                                data_item['content_summary'] = text_analysis_data.get('content_summary', subtitle_text)
                                                
                                                logger.info(f"å­—å¹•ç‰‡æ®µ {i+1} æ–‡æœ¬åˆ†æå®Œæˆ")
                                            else:
                                                logger.error(f"å­—å¹•ç‰‡æ®µ {i+1} æ–‡æœ¬åˆ†æJSONè§£æå¤±è´¥")
                                                # ä½¿ç”¨åŸå§‹å“åº”ä½œä¸ºæè¿°
                                                data_item['scene_description'] = f"æ–‡æœ¬åˆ†æç»“æœï¼š{text_response[:200]}..."
                                                data_item['emotion_tone'] = "ä¸­æ€§"
                                                data_item['content_summary'] = f"åŸºäºå­—å¹•ï¼š{subtitle_text}"
                                            
                                        except Exception as text_parse_error:
                                            logger.error(f"è§£æå­—å¹•ç‰‡æ®µ {i+1} çš„æ–‡æœ¬åˆ†æç»“æœå¤±è´¥: {text_parse_error}")
                                            # ä½¿ç”¨åŸå§‹å“åº”ä½œä¸ºæè¿°
                                            data_item['scene_description'] = f"æ–‡æœ¬åˆ†æç»“æœï¼š{text_response[:200]}..."
                                            data_item['emotion_tone'] = "ä¸­æ€§"
                                            data_item['content_summary'] = f"åŸºäºå­—å¹•ï¼š{subtitle_text}"
                                    else:
                                        logger.warning(f"å­—å¹•ç‰‡æ®µ {i+1} æ–‡æœ¬åˆ†æå¤±è´¥ï¼Œæœªè¿”å›ç»“æœ")
                                        data_item['scene_description'] = f"åŸºäºå­—å¹•æ¨æµ‹ï¼š{subtitle_text}"
                                        data_item['emotion_tone'] = "ä¸­æ€§"
                                        data_item['content_summary'] = f"åŸºäºå­—å¹•ï¼š{subtitle_text}"
                                        
                            finally:
                                # æ›´æ–°è¿›åº¦
                                completed_count['value'] += 1
                                current_progress = 40 + (completed_count['value'] / total_tasks) * 20  # 40%-60%çš„è¿›åº¦èŒƒå›´
                                update_progress(current_progress, f"å·²å®Œæˆ {completed_count['value']}/{total_tasks} ä¸ªå­—å¹•ç‰‡æ®µåˆ†æ...")

                    # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
                    tasks = [
                        analyze_single_segment(i, data_item) 
                        for i, data_item in enumerate(subtitle_keyframe_data)
                    ]
                    
                    # æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡å¹¶ç­‰å¾…å®Œæˆ
                    await asyncio.gather(*tasks)

                # è¿è¡Œå¹¶è¡Œåˆ†æ
                loop.run_until_complete(parallel_analyze_segments())
                
                # å…³é—­äº‹ä»¶å¾ªç¯
                loop.close()
                logger.info(f"å®Œæˆ {len(subtitle_keyframe_data)} ä¸ªå­—å¹•ç‰‡æ®µçš„ç”»é¢ç†è§£ä¸å‰§æƒ…æ¢³ç†")

                """
                5. ä¸»é¢˜æå–
                """
                update_progress(65, "æ­£åœ¨è¿›è¡Œä¸»é¢˜æå–...")
                
                # ä»é…ç½®ä¸­è·å–æ–‡æœ¬ç”Ÿæˆç›¸å…³é…ç½®  
                text_provider = config.app.get('text_llm_provider', 'gemini').lower()
                text_api_key = config.app.get(f'text_{text_provider}_api_key')
                text_model = config.app.get(f'text_{text_provider}_model_name')
                text_base_url = config.app.get(f'text_{text_provider}_base_url')
                themes_analyzer = create_text_analyzer(
                    provider=text_provider,
                    api_key=text_api_key,
                    model=text_model,
                    base_url=text_base_url
                )
                
                
                # æ•´åˆæ‰€æœ‰å­—å¹•å’Œç”»é¢ç†è§£ä¿¡æ¯
                combined_content = []
                for i, data_item in enumerate(subtitle_keyframe_data):
                    content_block = {
                        "time": data_item['timestamp'],
                        "subtitle": data_item['subtitle_text'],
                        "scene_description": data_item.get('scene_description', ''),
                        "content_summary": data_item.get('content_summary', '')
                    }
                    combined_content.append(content_block)
                
                # æ„å»ºä¸»é¢˜æå–prompt
                content_summary = "\n".join([
                    f"æ—¶é—´æ®µ: {item['time']}\nå­—å¹•: {item['subtitle']}\nç”»é¢ç†è§£: {item['scene_description']}\nå†…å®¹æ€»ç»“: {item['content_summary']}\n---"
                    for item in combined_content
                ])
                
                theme_extraction_prompt = f"""
                    åŸºäºä»¥ä¸‹è§†é¢‘å†…å®¹çš„å­—å¹•å’Œç”»é¢ç†è§£ï¼Œè¯·æå–å‡ºè¿™ä¸ªè§†é¢‘çš„ä¸»è¦ä¸»é¢˜ã€‚

                    {content_summary}

                    è¯·åˆ†æè§†é¢‘çš„æ ¸å¿ƒä¸»é¢˜ï¼Œå¹¶æŒ‰é‡è¦æ€§æ’åºã€‚æ¯ä¸ªä¸»é¢˜åº”è¯¥åŒ…å«ä¸»é¢˜åç§°å’Œè¯¦ç»†æè¿°ã€‚

                    è¯·åŠ¡å¿…ä½¿ç”¨ JSON æ ¼å¼è¾“å‡ºï¼š
                    {{
                    "themes": [
                        {{
                            "theme_name": "ä¸»é¢˜åç§°",
                            "theme_description": "ä¸»é¢˜çš„è¯¦ç»†æè¿°",
                            "relevance_score": 0.95
                        }},
                        {{
                            "theme_name": "æ¬¡è¦ä¸»é¢˜åç§°", 
                            "theme_description": "æ¬¡è¦ä¸»é¢˜çš„è¯¦ç»†æè¿°",
                            "relevance_score": 0.80
                        }}
                    ]
                    }}

                    è¯·åªè¿”å› JSON å­—ç¬¦ä¸²ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–è§£é‡Šæ€§æ–‡å­—ã€‚
                """
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                # TODO:è°ƒç”¨å¯èƒ½å›å­˜åœ¨è¿”å›å€¼æ ¼å¼ä¸å¤ªå¯¹çš„é—®é¢˜ï¼Œå› ä¸ºæ˜¯å±‚å±‚ä¿®æ”¹çš„ã€‚
                theme_response = loop.run_until_complete(
                    themes_analyzer.analyze_themes(
                        theme_extraction_prompt
                    )
                )
                loop.close()

                # è§£æä¸»é¢˜æå–ç»“æœ
                themes = []
                themes_data = parse_and_fix_json(theme_response)
                
                if themes_data:
                    themes = themes_data.get('themes', [])
                    
                    logger.info(f"æˆåŠŸæå– {len(themes)} ä¸ªä¸»é¢˜")
                    for theme in themes:
                        logger.info(f"ä¸»é¢˜: {theme.get('theme_name', '')} (ç›¸å…³åº¦: {theme.get('relevance_score', 0)})")
                else:
                    logger.error(f"ä¸»é¢˜æå–ç»“æœè§£æå¤±è´¥")
                    # åˆ›å»ºé»˜è®¤ä¸»é¢˜
                    themes = [{
                        "theme_name": "é»˜è®¤ä¸»é¢˜",
                        "theme_description": "åŸºäºè§†é¢‘å†…å®¹çš„ç»¼åˆä¸»é¢˜",
                        "relevance_score": 1.0
                    }]
                
                """
                6. ç”Ÿæˆè§£è¯´æ–‡æ¡ˆï¼ˆé’ˆå¯¹å¥åº·è§†é¢‘ä¼˜åŒ–ï¼‰
                """
                logger.info("å¼€å§‹ç”Ÿæˆå¥åº·è§†é¢‘è§£è¯´æ–‡æ¡ˆ")
                update_progress(80, "æ­£åœ¨ç”Ÿæˆè§£è¯´æ–‡æ¡ˆ...")
                
                # å¯¼å…¥è§£è¯´æ–‡æ¡ˆç”Ÿæˆå‡½æ•°
                from app.services.generate_narration_script import generate_narration
                
                # å‡†å¤‡åˆ†ææ•°æ®æ–‡ä»¶
                analysis_dir = os.path.join(utils.storage_dir(), "temp", "analysis")
                os.makedirs(analysis_dir, exist_ok=True)
                
                # ä½¿ç”¨å½“å‰æ—¶é—´åˆ›å»ºæ–‡ä»¶å
                now = datetime.now()
                timestamp_str = now.strftime("%Y%m%d_%H%M")
                
                # åˆ›å»ºä¸“é—¨é’ˆå¯¹å¥åº·è§†é¢‘çš„markdownè½¬æ¢å‡½æ•°
                def parse_health_video_to_markdown(subtitle_keyframe_data, themes):
                    """
                    å°†å¥åº·è§†é¢‘çš„å­—å¹•å’Œç”»é¢åˆ†ææ•°æ®è½¬æ¢ä¸ºMarkdownæ ¼å¼
                    é’ˆå¯¹å¥åº·è§†é¢‘çš„ç‰¹ç‚¹è¿›è¡Œä¼˜åŒ–
                    """
                    markdown = "# å¥åº·è§†é¢‘å†…å®¹åˆ†æ\n\n"
                    
                    # æ·»åŠ ä¸»é¢˜ä¿¡æ¯
                    if themes:
                        markdown += "## è§†é¢‘ä¸»é¢˜\n"
                        for i, theme in enumerate(themes[:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ªä¸»é¢˜
                            theme_name = theme.get('theme_name', f'ä¸»é¢˜{i}')
                            theme_desc = theme.get('theme_description', '')
                            relevance = theme.get('relevance_score', 0)
                            markdown += f"- **{theme_name}** (ç›¸å…³åº¦: {relevance:.2f}): {theme_desc}\n"
                        markdown += "\n"
                    
                    # å¤„ç†æ¯ä¸ªå­—å¹•ç‰‡æ®µ
                    for i, data_item in enumerate(subtitle_keyframe_data, 1):
                        timestamp = data_item['timestamp']
                        subtitle_text = data_item['subtitle_text']
                        scene_description = data_item.get('scene_description', '')
                        emotion_tone = data_item.get('emotion_tone', '')
                        key_elements = data_item.get('key_elements', [])
                        plot_analysis = data_item.get('plot_analysis', '')
                        content_summary = data_item.get('content_summary', '')
                        
                        markdown += f"## ç‰‡æ®µ {i}\n"
                        markdown += f"- **æ—¶é—´èŒƒå›´**: {timestamp}\n"
                        markdown += f"- **åŸå§‹å­—å¹•**: {subtitle_text}\n"
                        
                        if scene_description:
                            markdown += f"- **ç”»é¢æè¿°**: {scene_description}\n"
                        
                        if emotion_tone:
                            markdown += f"- **æƒ…æ„Ÿè‰²å½©**: {emotion_tone}\n"
                        
                        if key_elements:
                            elements_str = "ã€".join(key_elements)
                            markdown += f"- **å…³é”®è¦ç´ **: {elements_str}\n"
                        
                        if plot_analysis:
                            markdown += f"- **å†…å®¹åˆ†æ**: {plot_analysis}\n"
                        
                        if content_summary:
                            markdown += f"- **ç‰‡æ®µæ€»ç»“**: {content_summary}\n"
                        
                        markdown += "\n"
                    
                    return markdown
                
                # ç”Ÿæˆä¸“é—¨é’ˆå¯¹å¥åº·è§†é¢‘çš„markdownå†…å®¹
                markdown_output = parse_health_video_to_markdown(subtitle_keyframe_data, themes)
                
                # ä¿å­˜markdownå†…å®¹ä»¥ä¾¿è°ƒè¯•
                markdown_file = os.path.join(analysis_dir, f"health_video_markdown_{timestamp_str}.md")
                with open(markdown_file, 'w', encoding='utf-8') as f:
                    f.write(markdown_output)
                logger.info(f"Markdownå†…å®¹å·²ä¿å­˜åˆ°: {markdown_file}")
                
                # ä»é…ç½®ä¸­è·å–æ–‡æœ¬ç”Ÿæˆç›¸å…³é…ç½®
                text_provider = config.app.get('text_llm_provider', 'gemini').lower()
                text_api_key = config.app.get(f'text_{text_provider}_api_key')
                text_model = config.app.get(f'text_{text_provider}_model_name')
                text_base_url = config.app.get(f'text_{text_provider}_base_url')
                
                llm_params = {
                    "text_provider": text_provider,
                    "text_api_key": text_api_key,
                    "text_model_name": text_model,
                    "text_base_url": text_base_url
                }
                check_video_config(llm_params)
                
                # ç”Ÿæˆè§£è¯´æ–‡æ¡ˆ - ä¿æŒä¸åŸæœ‰å‡½æ•°çš„å…¼å®¹æ€§
                narration = generate_narration(
                    markdown_output,
                    text_api_key,
                    base_url=text_base_url,
                    model=text_model
                )
                
                # ä½¿ç”¨å¢å¼ºçš„JSONè§£æå™¨
                narration_data = parse_and_fix_json(narration)
                
                if not narration_data or 'items' not in narration_data:
                    logger.error(f"è§£è¯´æ–‡æ¡ˆJSONè§£æå¤±è´¥ï¼ŒåŸå§‹å†…å®¹: {narration[:200]}...")
                    raise Exception("è§£è¯´æ–‡æ¡ˆæ ¼å¼é”™è¯¯ï¼Œæ— æ³•è§£æJSONæˆ–ç¼ºå°‘itemså­—æ®µ")
                
                narration_dict = narration_data['items']
                # ä¸º narration_dict ä¸­æ¯ä¸ª item æ–°å¢ä¸€ä¸ª OST: 2 çš„å­—æ®µ, ä»£è¡¨ä¿ç•™åŸå£°å’Œé…éŸ³
                narration_dict = [{**item, "OST": 2} for item in narration_dict]
                logger.info(f"è§£è¯´æ–‡æ¡ˆç”Ÿæˆå®Œæˆï¼Œå…± {len(narration_dict)} ä¸ªç‰‡æ®µ")
                
                # ç»“æœè½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
                script = json.dumps(narration_dict, ensure_ascii=False, indent=2)

                """
                7. ä¿å­˜ç»“æœ
                """
                update_progress(90, "æ­£åœ¨ä¿å­˜ç»“æœ...")
                
                # ç¡®ä¿åˆ†æç›®å½•å­˜åœ¨ï¼ˆå·²åœ¨ä¸Šé¢åˆ›å»ºï¼‰
                
                # ä¿å­˜å®Œæ•´çš„æ•°æ®ç»“æ„
                primary_theme = themes[0] if themes else {
                    "theme_name": "å¥åº·è§†é¢‘ä¸»é¢˜",
                    "theme_description": "åŸºäºè§†é¢‘å†…å®¹çš„å¥åº·ç›¸å…³ä¸»é¢˜",
                    "relevance_score": 1.0
                }
                
                full_data = {
                    "subtitle_segments": subtitle_keyframe_data,
                    "themes": themes,
                    "primary_theme": primary_theme,
                    "script_items": json.loads(script)
                }
                
                final_analysis_file = os.path.join(analysis_dir, f"health_video_final_analysis_{timestamp_str}.json")
                with open(final_analysis_file, 'w', encoding='utf-8') as f:
                    json.dump(full_data, f, ensure_ascii=False, indent=2)
                
                # ä¿å­˜è„šæœ¬æ–‡ä»¶
                script_file = os.path.join(analysis_dir, f"health_video_script_{timestamp_str}.json")
                with open(script_file, 'w', encoding='utf-8') as f:
                    f.write(script)
                
                logger.info(f"å®Œæ•´åˆ†æç»“æœå·²ä¿å­˜åˆ°: {final_analysis_file}")
                logger.info(f"è§£è¯´è„šæœ¬å·²ä¿å­˜åˆ°: {script_file}")
                
                update_progress(100, "å¤„ç†å®Œæˆï¼")
                logger.info("å¥åº·è§†é¢‘è„šæœ¬ç”Ÿæˆä»»åŠ¡å®Œæˆ")

            except Exception as e:
                logger.exception(f"å¤§æ¨¡å‹å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯\n{traceback.format_exc()}")
                raise Exception(f"åˆ†æå¤±è´¥: {str(e)}")

            if script is None:
                st.error("ç”Ÿæˆè„šæœ¬å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
                st.stop()
                
            logger.info(f"å¥åº·è§†é¢‘è§£è¯´è„šæœ¬ç”Ÿæˆå®Œæˆ")
            
            if isinstance(script, list):
                st.session_state['video_clip_json'] = script
            elif isinstance(script, str):
                st.session_state['video_clip_json'] = json.loads(script)
                
            update_progress(100, "è„šæœ¬ç”Ÿæˆå®Œæˆ")

        time.sleep(0.1)
        progress_bar.progress(100)
        status_text.text("ğŸ‰ è„šæœ¬ç”Ÿæˆå®Œæˆï¼")
        st.success("âœ… è§†é¢‘è„šæœ¬ç”ŸæˆæˆåŠŸï¼")

    except Exception as err:
        st.error(f"âŒ ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(err)}")
        logger.exception(f"ç”Ÿæˆè„šæœ¬æ—¶å‘ç”Ÿé”™è¯¯\n{traceback.format_exc()}")
        return None
    finally:
        time.sleep(2)
        progress_bar.empty()
        status_text.empty()
        
def parse_and_fix_json(json_string):
    """
    è§£æå¹¶ä¿®å¤JSONå­—ç¬¦ä¸²

    Args:
        json_string: å¾…è§£æçš„JSONå­—ç¬¦ä¸²

    Returns:
        dict: è§£æåçš„å­—å…¸ï¼Œå¦‚æœè§£æå¤±è´¥è¿”å›None
    """
    if not json_string or not json_string.strip():
        logger.error("JSONå­—ç¬¦ä¸²ä¸ºç©º")
        return None

    # æ¸…ç†å­—ç¬¦ä¸²
    json_string = json_string.strip()

    # å°è¯•ç›´æ¥è§£æ
    try:
        return json.loads(json_string)
    except json.JSONDecodeError as e:
        logger.warning(f"ç›´æ¥JSONè§£æå¤±è´¥: {e}")

    # å°è¯•ä¿®å¤åŒå¤§æ‹¬å·é—®é¢˜ï¼ˆLLMç”Ÿæˆçš„å¸¸è§é—®é¢˜ï¼‰
    try:
        # å°†åŒå¤§æ‹¬å·æ›¿æ¢ä¸ºå•å¤§æ‹¬å·
        fixed_braces = json_string.replace('{{', '{').replace('}}', '}')
        logger.info("ä¿®å¤åŒå¤§æ‹¬å·æ ¼å¼, jsonè§£ææˆåŠŸ")
        return json.loads(fixed_braces)
    except json.JSONDecodeError:
        pass

    # å°è¯•æå–JSONéƒ¨åˆ†
    try:
        # æŸ¥æ‰¾JSONä»£ç å—
        json_match = re.search(r'```json\s*(.*?)\s*```', json_string, re.DOTALL)
        if json_match:
            json_content = json_match.group(1).strip()
            logger.info("ä»ä»£ç å—ä¸­æå–JSONå†…å®¹")
            return json.loads(json_content)
    except json.JSONDecodeError:
        pass

    # å°è¯•æŸ¥æ‰¾å¤§æ‹¬å·åŒ…å›´çš„å†…å®¹
    try:
        # æŸ¥æ‰¾ç¬¬ä¸€ä¸ª { åˆ°æœ€åä¸€ä¸ª } çš„å†…å®¹
        start_idx = json_string.find('{')
        end_idx = json_string.rfind('}')
        if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
            json_content = json_string[start_idx:end_idx+1]
            logger.info("æå–å¤§æ‹¬å·åŒ…å›´çš„JSONå†…å®¹")
            return json.loads(json_content)
    except json.JSONDecodeError:
        pass

    # å°è¯•ç»¼åˆä¿®å¤JSONæ ¼å¼é—®é¢˜
    try:
        fixed_json = json_string

        # 1. ä¿®å¤åŒå¤§æ‹¬å·é—®é¢˜
        fixed_json = fixed_json.replace('{{', '{').replace('}}', '}')

        # 2. æå–JSONå†…å®¹ï¼ˆå¦‚æœæœ‰å…¶ä»–æ–‡æœ¬åŒ…å›´ï¼‰
        start_idx = fixed_json.find('{')
        end_idx = fixed_json.rfind('}')
        if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
            fixed_json = fixed_json[start_idx:end_idx+1]

        # 3. ç§»é™¤æ³¨é‡Š
        fixed_json = re.sub(r'#.*', '', fixed_json)
        fixed_json = re.sub(r'//.*', '', fixed_json)

        # 4. ç§»é™¤å¤šä½™çš„é€—å·
        fixed_json = re.sub(r',\s*}', '}', fixed_json)
        fixed_json = re.sub(r',\s*]', ']', fixed_json)

        # 5. ä¿®å¤å•å¼•å·
        fixed_json = re.sub(r"'([^']*)':", r'"\1":', fixed_json)

        # 6. ä¿®å¤æ²¡æœ‰å¼•å·çš„å±æ€§å
        fixed_json = re.sub(r'(\w+)(\s*):', r'"\1"\2:', fixed_json)

        # 7. ä¿®å¤é‡å¤çš„å¼•å·
        fixed_json = re.sub(r'""([^"]*?)""', r'"\1"', fixed_json)

        logger.info("å°è¯•ç»¼åˆä¿®å¤JSONæ ¼å¼é—®é¢˜åè§£æ")
        return json.loads(fixed_json)
    except json.JSONDecodeError as e:
        logger.debug(f"ç»¼åˆä¿®å¤å¤±è´¥: {e}")
        pass

    # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›None
    logger.error(f"æ‰€æœ‰JSONè§£ææ–¹æ³•éƒ½å¤±è´¥ï¼ŒåŸå§‹å†…å®¹: {json_string[:200]}...")
    return None